
@Article{	  Aka69,
  author	= {Akaike, Hirotugu},
  journal	= {Annals of the Institute of Statistical Mathematics},
  number	= 1,
  pages		= {243-247},
  publisher	= {Springer},
  title		= {Fitting autoregressive models for prediction},
  url		= {http://link.springer.com/content/pdf/10.1007/BF02532251.pdf},
  volume	= 21,
  year		= 1969
}

@InProceedings{	  BM+20,
  author	= {Tom Brown and Benjamin Mann and Nick Ryder and Melanie
		  Subbiah and others},
  title		= {Language models are few-shot learners},
  year		= 2020,
  booktitle	= {Advances in Neural Information Processing Systems 33
		  (NeurIPS 2020)},
  editor	= {H. Larochelle and M. Ranzato and R. Hadsell and M. F.
		  Balcan and H. Lin},
  pages		= {1877-1901},
  url		= {http://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf}
}

@Article{	  BR92,
  author	= {Avrim L. Blum and Ronald L. Rivest},
  journal	= {Neural Networks},
  number	= 1,
  pages		= {117-127},
  title		= {Training a 3-node neural network is {NP}-complete},
  url		= {http://doi.org/10.1016/S0893-6080(05)80010-3},
  volume	= 5,
  year		= 1992
}

@Article{	  BSF94,
  author	= {Bengio, Yoshua and Simard, Patrice and Frasconi, Paolo},
  issn		= {1045-9227},
  journal	= {IEEE Transactions on Neural Networks},
  number	= 2,
  pages		= {157-166},
  publisher	= {IEEE Press},
  title		= {Learning Long-Term Dependencies with Gradient Descent is
		  Difficult},
  url		= {http://doi.org/10.1109/72.279181},
  volume	= 5,
  year		= 1994
}

@Manual{	  CDF+03,
  author	= {Chen, Mao and Dorer, Klaus and Foroughi, Ehsan and Heintz,
		  Fredrick and Huang, ZhanXiang and Kapetanakis, Spiros and
		  Kostiadis, Kostas and Kummeneje, Johan and Murray, Jan and
		  Noda, Itsuki and Obst, Oliver and Riley, Pat and Steffens,
		  Timo and Wang, Yi and Yin, Xiang},
  month		= {February},
  organization	= {The {R}obo{C}up Federation},
  title		= {Users Manual: {R}obo{C}up Soccer Server -- for Soccer
		  Server Version 7.07 and Later},
  url		= {http://helios.hampshire.edu/jdavila/cs278/virtual_worlds/robocup_manual-20030211.pdf},
  year		= {2003}
}

@Book{		  CK14,
  address	= {Providence, Rhode Island},
  author	= {Fritz Colonius and Wolfgang Kliemann},
  publisher	= {American Mathematical Society},
  series	= {Graduate Studies in Mathematics},
  title		= {Dynamical Systems and Linear Algebra},
  url		= {http://doi.org/10.1090/gsm/158},
  volume	= 158,
  year		= 2014
}

@Article{	  CSB21,
  title		= {Encoding-based memory for recurrent neural networks},
  journal	= {Neurocomputing},
  volume	= 456,
  pages		= {407-420},
  year		= 2021,
  issn		= {0925-2312},
  url		= {http://doi.org/10.1016/j.neucom.2021.04.051},
  author	= {Antonio Carta and Alessandro Sperduti and Davide Bacciu}
}

@Article{	  CW+16,
  author	= {Romain Couillet and Gilles Wainrib and Harry Sevi and
		  Hafiz Tiomoko Ali},
  title		= {The Asymptotic Performance of Linear Echo State Neural
		  Networks},
  journal	= {Journal of Machine Learning Research},
  year		= 2016,
  volume	= 17,
  number	= 178,
  pages		= {1-35},
  url		= {http://jmlr.org/papers/v17/16-076.html}
}

@Article{	  DDH07,
  author	= {Demmel, James and Dumitriu, Ioana and Holtz, Olga},
  issn		= {0945-3245},
  journal	= {Numerische Mathematik},
  number	= 1,
  pages		= {59-91},
  title		= {Fast linear algebra is stable},
  url		= {http://doi.org/10.1007/s00211-007-0114-x},
  volume	= 108,
  year		= 2007
}

@Article{	  DY14,
  author	= {Deng, Li and Yu, Dong},
  journal	= {Foundations and Trends in Signal Processing},
  number	= {3-4},
  pages		= {198-387},
  title		= {Deep Learning: Methods and Applications},
  url		= {http://research.microsoft.com/pubs/209355/DeepLearning-NowPublishing-Vol7-SIG-039.pdf},
  volume	= 7,
  year		= 2014
}

@Manual{	  EB+17,
  author	= {Eaton, John Wesley and Bateman, David and Hauberg,
		  S{\o}ren and Wehbring, Rik},
  note		= {Edition 4 for Octave version 4.2.1},
  title		= {{GNU} {O}ctave -- A High-Level Interactive Language for
		  Numerical Computations},
  url		= {http://www.octave.org/},
  year		= 2017
}

@Article{	  Elm90,
  author	= {Jeffrey L. Elman},
  issn		= {0364-0213},
  journal	= {Cognitive Science},
  pages		= {179-211},
  title		= {Finding Structure in Time},
  url		= {http://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1},
  volume	= 14,
  year		= 1990
}

@Book{		  GBC16,
  address	= {Cambridge, MA, London},
  author	= {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  publisher	= {MIT Press},
  series	= {Adaptive Computation and Machine Learning},
  title		= {Deep Learning},
  url		= {http://www.deeplearningbook.org},
  year		= 2016
}

@InProceedings{	  GFG16,
  author	= {Thomas Gabel and Egbert Falkenberg and Eicke Godehardt},
  editor	= {Sven Behnke and Raymond Sheh and Sanem Sariel and Daniel
		  D. Lee},
  title		= {Progress in {R}obo{C}up Revisited: The State of Soccer
		  Simulation 2{D}},
  booktitle	= {RoboCup 2016: Robot Soccer World Cup XX. RoboCup
		  International Symposium},
  address	= {Leipzig},
  series	= {Lecture Notes in Computer Science},
  volume	= 9776,
  pages		= {144-156},
  publisher	= {Springer Nature Switzerland},
  year		= 2017,
  url		= {http://doi.org/10.1007/978-3-319-68792-6_12}
}

@Article{	  GR69,
  title		= {Minimum spanning trees and single linkage cluster
		  analysis},
  author	= {Gower, John C. and Ross, Gavin J. S.},
  journal	= {Journal of the Royal Statistical Society: Series C
		  (Applied Statistics)},
  volume	= 18,
  number	= 1,
  pages		= {54-64},
  year		= 1969,
  publisher	= {Wiley Online Library},
  url		= {http://www.jstor.org/stable/2346439}
}

@InProceedings{	  GW13,
  author	= {Stefan Gl{\"u}ge and Andreas Wendemuth},
  booktitle	= {Natural and Artificial Models in Computation and Biology
		  -- 5th International Work-Conference on the Interplay
		  Between Natural and Artificial Computation, {IWINAC} 2013.
		  Proceedings, Part {I}},
  editor	= {Ferr{\'a}ndez de Vicente, Jos{\'e} Manuel and {\'A}lvarez
		  S{\'a}nchez, Jos{\'e} Ram{\'o}n and de la Paz L{\'o}pez,
		  F{\'e}lix and Toledo{-}Moreo, F. Javier},
  pages		= {412-420},
  publisher	= {Springer},
  series	= {LNCS~7930},
  title		= {Solving Number Series with Simple Recurrent Networks},
  url		= {http://doi.org/10.1007/978-3-642-38637-4_43},
  year		= 2013
}

@Book{		  HA13,
  title		= {Forecasting: principles and practices},
  publisher	= {OTexts},
  year		= 2013,
  author	= {Rob J. Hyndman and George Athanasopoulos},
  address	= {Melbourne, Australia},
  url		= {http://otexts.com/fpp2/}
}

@Article{	  Ham00,
  abstract	= {The capability of recurrent neural networks of
		  approximating functions from lists of real vectors to a
		  real vector space is examined. Any measurable function can
		  be approximated in probability. Additionally, bounds on the
		  resources sufficient for an approximation can be derived in
		  interesting cases. On the contrary, there exist computable
		  mappings on symbolic data which cannot be approximated in
		  the maximum norm. For restricted input length, some
		  continuous functions on real-valued sequences need a number
		  of neurons increasing at least linearly in the input
		  length. On unary sequences, any mapping with bounded range
		  can be approximated in the maximum norm. Consequently,
		  standard sigmoidal networks can compute any mapping on
		  offline inputs as a computational model.},
  author	= {Barbara Hammer},
  journal	= {Neurocomputing},
  keywords	= {Recurrent neural networks, Universal approximation,
		  Sigmoidal networks, Computational capability},
  number	= {1-4},
  pages		= {107-123},
  title		= {On the approximation capability of recurrent neural
		  networks},
  volume	= 31,
  year		= 2000,
  url		= {http://doi.org/10.1016/S0925-2312(99)00174-5}
}

@Book{		  HH17,
  address	= {Philadelphia, PA},
  author	= {Higham, Desmond J. and Higham, Nicholas J.},
  edition	= {3rd},
  publisher	= {Siam},
  title		= {MatLab Guide},
  url		= {http://bookstore.siam.org/ot150/},
  year		= 2017
}

@Book{		  HJ13,
  address	= {New York, NY},
  author	= {Roger A. Horn and Charles R. Johnson},
  edition	= {2nd},
  publisher	= {Cambridge University Press},
  title		= {Matrix Analysis},
  year		= 2013,
  url		= {http://www.cse.zju.edu.cn/eclass/attachments/2015-10/01-1446086008-145421.pdf}
}

@Article{	  HK08,
  author	= {Rob J. Hyndman and Yeasmin Khandakar},
  title		= {Automatic Time Series Forecasting: The Forecast Package
		  for {R}},
  journal	= {Journal of Statistical Software},
  year		= 2008,
  volume	= 27,
  number	= 3,
  url		= {http://doi.org/10.18637/jss.v027.i03}
}

@InCollection{	  Hog13,
  address	= {Boca Raton, FL},
  author	= {Leslie Hogben},
  booktitle	= {Handbook of Linear Algebra},
  chapter	= 6,
  edition	= {2nd},
  publisher	= {Chapman and Hall/CRC},
  series	= {Discrete Mathematics and Its Applications},
  title		= {Canonical Forms},
  year		= 2013
}

@Article{	  Hor91,
  author	= {Kurt Hornik},
  issn		= {0893-6080},
  journal	= {Neural Networks},
  number	= 2,
  pages		= {251-257},
  title		= {Approximation capabilities of multilayer feedforward
		  networks},
  url		= {http://doi.org/10.1016/0893-6080(91)90009-T},
  volume	= 4,
  year		= 1991
}

@InProceedings{	  HQ17,
  abstract	= {Modeling temporal sequences plays a fundamental role in
		  various modern applications and has drawn more and more
		  attentions in the machine learning community. Among those
		  efforts on improving the capability to represent temporal
		  data, the Long Short-Term Memory (LSTM) has achieved great
		  success in many areas. Although the LSTM can capture
		  long-range dependency in the time domain, it does not
		  explicitly model the pattern occurrences in the frequency
		  domain that plays an important role in tracking and
		  predicting data points over various time cycles. We propose
		  the State-Frequency Memory (SFM), a novel recurrent
		  architecture that allows to separate dynamic patterns
		  across different frequency components and their impacts on
		  modeling the temporal contexts of input sequences. By
		  jointly decomposing memorized dynamics into state-frequency
		  components, the SFM is able to offer a fine-grained
		  analysis of temporal sequences by capturing the dependency
		  of uncovered patterns in both time and frequency domains.
		  Evaluations on several temporal modeling tasks demonstrate
		  the SFM can yield competitive performances, in particular
		  as compared with the state-of-the-art LSTM models.},
  address	= {Sydney, Australia},
  author	= {Hao Hu and Guo-Jun Qi},
  booktitle	= {Proceedings of the 34th International Conference on
		  Machine Learning},
  editor	= {Doina Precup and Yee Whye Teh},
  pages		= {1568-1577},
  publisher	= {PMLR},
  series	= {Proceedings of Machine Learning Research},
  title		= {State-Frequency Memory Recurrent Neural Networks},
  url		= {http://proceedings.mlr.press/v70/hu17c.html},
  volume	= 70,
  year		= 2017
}

@Article{	  HS97,
  address	= {Cambridge, MA, USA},
  author	= {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  issn		= {0899-7667},
  journal	= {Neural Computation},
  number	= 8,
  pages		= {1735-1780},
  publisher	= {MIT Press},
  title		= {Long Short-Term Memory},
  url		= {http://doi.org/10.1162/neco.1997.9.8.1735},
  volume	= 9,
  year		= 1997
}

@Article{	  IL+11,
  abstract	= {Hardware implementations of spiking neurons can be
		  extremely useful for a large variety of applications,
		  ranging from high-speed modeling of large-scale neural
		  systems to real-time behaving systems, to bidirectional
		  brain-machine interfaces. The specific circuit solutions
		  used to implement silicon neurons depend on the application
		  requirements. In this paper we describe the most common
		  building blocks and techniques used to implement these
		  circuits, and present an overview of a wide range of
		  neuromorphic silicon neurons, which implement different
		  computational models, ranging from biophysically realistic
		  and conductance based Hodgkin-Huxley models to
		  bi-dimensional generalized adaptive Integrate and Fire
		  models. We compare the different design methodologies used
		  for each silicon neuron design described, and demonstrate
		  their features with experimental results, measured from a
		  wide range of fabricated VLSI chips.},
  author	= {Indiveri, Giacomo and Linares-Barranco, Bernabe and
		  Hamilton, Tara and van Schaik, Andr{\'e} and
		  Etienne-Cummings, Ralph and Delbruck, Tobi and Liu,
		  Shih-Chii and Dudek, Piotr and H{\"a}fliger, Philipp and
		  Renaud, Sylvie and Schemmel, Johannes and Cauwenberghs,
		  Gert and Arthur, John and Hynna, Kai and Folowosele,
		  Fopefolu and Sa{\"\i}ghi, Sylvain and Serrano-Gotarredona,
		  Teresa and Wijekoon, Jayawan and Wang, Yingxue and Boahen,
		  Kwabena},
  issn		= {1662-453X},
  journal	= {Frontiers in Neuroscience},
  pages		= 73,
  title		= {Neuromorphic Silicon Neuron Circuits},
  url		= {http://www.frontiersin.org/article/10.3389/fnins.2011.00073},
  volume	= 5,
  year		= 2011
}

@Article{	  Jae07,
  author	= {Jaeger, Herbert},
  journal	= {Scholarpedia},
  note		= {Revision \#151757},
  number	= 9,
  pages		= 2330,
  title		= {Echo state network},
  url		= {http://doi.org/10.4249/scholarpedia.2330},
  volume	= 2,
  year		= 2007
}

@TechReport{	  Jae14,
  author	= {Herbert Jaeger},
  institution	= {Cornell University Library},
  number	= {abs/1403.3369},
  title		= {Controlling Recurrent Neural Networks by Conceptors},
  type		= {{CoRR} -- Computing Research Repository},
  url		= {http://arxiv.org/abs/1403.3369},
  year		= 2014
}

@Article{	  Jae17,
  title		= {Using conceptors to manage neural long-term memories for
		  temporal patterns},
  author	= {Jaeger, Herbert},
  journal	= {Journal of Machine Learning Research},
  volume	= 18,
  number	= 1,
  pages		= {387-429},
  year		= 2017,
  url		= {http://dl.acm.org/doi/abs/10.5555/3122009.3122022}
}

@Article{	  JH04,
  author	= {Jaeger, Herbert and Haas, Harald},
  journal	= {Science},
  number	= 304,
  pages		= {78-80},
  title		= {Harnessing nonlinearity: Predicting chaotic systems and
		  saving energy in wireless communication},
  volume	= 2,
  year		= 2004,
  url		= {http://doi.org/10.1126/science.1091277}
}

@InCollection{	  Jol11,
  title		= {Principal Component Analysis},
  author	= {Jolliffe, Ian},
  year		= 2011,
  publisher	= {Springer},
  editor	= {Lovric, Miodrag},
  booktitle	= {International Encyclopedia of Statistical Science},
  address	= {Berlin, Heidelberg},
  pages		= {1094-1096},
  isbn		= {978-3-642-04898-2},
  url		= {http://doi.org/10.1007/978-3-642-04898-2_455}
}

@Article{	  KA+97,
  author	= {Hiroaki Kitano and Minoru Asada and Yasuo Kuniyoshi and
		  Itsuki Noda and Eiichi Osawa and Hitoshi Matsubara},
  journal	= {{AI} Magazine},
  number	= 1,
  pages		= {73-85},
  title		= {{R}obo{C}up: {A} Challenge Problem for {AI}},
  volume	= 18,
  year		= 1997,
  url		= {http://www.aaai.org/ojs/index.php/aimagazine/article/view/1276/1177}
}

@Book{		  KB+16,
  author	= {Rudolf Kruse and Christian Borgelt and Christian Braune
		  and Sanaz Mostaghim and Matthias Steinbrecher},
  title		= {Computational Intelligence. A Methodological
		  Introduction},
  publisher	= {Springer},
  edition	= {2nd},
  year		= 2016,
  address	= {London},
  url		= {http://link.springer.com/book/10.1007/978-1-4471-7296-3}
}

@Article{	  KLB12,
  author	= {Danil Koryakin and Johannes Lohmann and Martin V. Butz},
  issn		= {0893-6080},
  journal	= {Neural Networks},
  pages		= {35-45},
  title		= {Balanced echo state networks},
  url		= {http://doi.org/10.1016/j.neunet.2012.08.008},
  volume	= 36,
  year		= 2012
}

@InProceedings{ KOS21b,
  author	= {Stefanie Krause and Oliver Otto and Frieder Stolzenburg},
  title		= {Fast Classification Learning with Neural Networks and Conceptors for Speech Recognition and Car Driving Maneuvers},
  pages		= {45-57},
  booktitle	= {Proceedings of the 14th Multi-Disciplinary International Conference on Artificial Intelligence (MIWAI)},
  editor	= {Phatthanaphong Chomphuwiset and Junmo Kim and Pornntiwa Pawara},
  year		= 2021,
  publisher	= {Springer Nature Switzerland},
  series	= {LNAI~12832},
  doi		= {10.1007/978-3-030-80253-0_5},
  url		= {http://link.springer.com/chapter/10.1007/978-3-030-80253-0_5}
}

@InProceedings{	  LAT19,
  title		= {{SNIP}: Single-shot network pruning based on connection
		  sensitivity},
  author	= {Lee, Namhoon and Thalaiyasingam Ajanthan and Philip H. S.
		  Torr},
  booktitle	= {International Conference on Learning Representations},
  year		= 2019,
  url		= {http://arxiv.org/abs/1810.02340}
}

@TechReport{	  LBE15,
  author	= {Zachary C. Lipton and John Berkowitz and Charles Elkan},
  institution	= {Cornell University Library},
  number	= {abs/1506.00019},
  title		= {A Critical Review of Recurrent Neural Networks for
		  Sequence Learning},
  type		= {{CoRR} -- Computing Research Repository},
  url		= {http://arxiv.org/abs/1506.00019},
  year		= 2015
}

@TechReport{	  Lit20,
  author	= {Sandra Litz},
  title		= {Predicting Stock Prices Using Recurrent Neural Networks},
  note		= {In German},
  address	= {Harz University of Applied Sciences},
  institution	= {Automation and Computer Sciences Department},
  year		= 2020,
  url		= {http://dx.doi.org/10.25673/35875},
  type		= {WAIT -- Werniger√∂der Automatisierungs- und
		  Informatiktexte},
  number	= {01/2020}
}

@InProceedings{	  LL+19,
  title		= {Transformer-Based Capsule Network For Stock Movement
		  Prediction},
  author	= {Liu, Jintao and Lin, Hongfei and Liu, Xikai and Xu, Bo and
		  Ren, Yuqi and Diao, Yufeng and Yang, Liang},
  booktitle	= {Proceedings of the First Workshop on Financial Technology
		  and Natural Language Processing},
  year		= 2019,
  address	= {Macao, China},
  url		= {http://www.aclweb.org/anthology/W19-5511},
  pages		= {66-73}
}

@Article{	  LL17,
  author	= {Liao, Yongbo and Li, Hongmei},
  journal	= {Global Journal of Researches in Engineering (F)},
  number	= 5,
  title		= {Reservoir Computing Trend on Software and Hardware
		  Implementation},
  url		= {http://engineeringresearch.org/index.php/GJRE/article/download/1654/1585},
  volume	= 17,
  year		= 2017
}

@Article{	  Mar17,
  abstract	= {Recurrent networks are trained to memorize their input
		  better, often in the hopes that such training will increase
		  the ability of the network to predict. We show that
		  networks designed to memorize input can be arbitrarily bad
		  at prediction. We also find, for several types of inputs,
		  that one-node networks optimized for prediction are nearly
		  at upper bounds on predictive capacity given by Wiener
		  filters and are roughly equivalent in performance to
		  randomly generated five-node networks. Our results suggest
		  that maximizing memory capacity leads to very different
		  networks than maximizing predictive capacity and that
		  optimizing recurrent weights can decrease reservoir size by
		  half an order of magnitude.},
  author	= {Sarah Marzen},
  journal	= {Physical Review E},
  number	= 3,
  pages		= {032308\,[1-7]},
  title		= {Difference between memory and prediction in linear
		  recurrent networks},
  url		= {http://doi.org/10.1103/PhysRevE.96.032308},
  volume	= 96,
  year		= 2017
}

@Article{	  Mea90,
  abstract	= {It is shown that for many problems, particularly those in
		  which the input data are ill-conditioned and the
		  computation can be specified in a relative manner,
		  biological solutions are many orders of magnitude more
		  effective than those using digital methods. This advantage
		  can be attributed principally to the use of elementary
		  physical phenomena as computational primitives, and to the
		  representation of information by the relative values of
		  analog signals rather than by the absolute values of
		  digital signals. This approach requires adaptive techniques
		  to mitigate the effects of component differences. This kind
		  of adaptation leads naturally to systems that learn about
		  their environment. Large-scale adaptive analog systems are
		  more robust to component degradation and failure than are
		  more conventional systems, and they use far less power. For
		  this reason, adaptive analog technology can be expected to
		  utilize the full potential of wafer-scale silicon
		  fabrication.},
  author	= {Carver Mead},
  journal	= {Proceedings of the IEEE},
  keywords	= {adaptive systems;analogue circuits;neural
		  nets;VLSI;VLSI;neuromorphic electronic systems;analogue
		  circuits;neural nets;analog signals;adaptive analog
		  systems;Neuromorphics;Biology computing;Analog
		  computers;Physics computing;Large-scale systems;Adaptive
		  systems;Robustness;Degradation;Silicon;Fabrication},
  month		= oct,
  number	= 10,
  pages		= {1629-1636},
  title		= {Neuromorphic electronic systems},
  volume	= 78,
  year		= 1990,
  url		= {http://ieeexplore.ieee.org/document/58356}
}

@Article{	  MJ13,
  abstract	= {The echo state property is a key for the design and
		  training of recurrent neural networks within the paradigm
		  of reservoir computing. In intuitive terms, this is a
		  passivity condition: a network having this property, when
		  driven by an input signal, will become entrained by the
		  input and develop an internal response signal. This excited
		  internal dynamics can be seen as a high-dimensional,
		  nonlinear, unique transform of the input with a rich memory
		  content. This view has implications for understanding
		  neural dynamics beyond the field of reservoir computing.
		  Available definitions and theorems concerning the echo
		  state property, however, are of little practical use
		  because they do not relate the network response to temporal
		  or statistical properties of the driving input. Here we
		  present a new definition of the echo state property that
		  directly connects it to such properties. We derive a
		  fundamental 0-1 law: if the input comes from an ergodic
		  source, the network response has the echo state property
		  with probability one or zero, independent of the given
		  network. Furthermore, we give a sufficient condition for
		  the echo state property that connects statistical
		  characteristics of the input to algebraic properties of the
		  network connection matrix. The mathematical methods that we
		  employ are freshly imported from the young field of
		  nonautonomous dynamical systems theory. Since these methods
		  are not yet well known in neural computation research, we
		  introduce them in some detail. As a side story, we hope to
		  demonstrate the eminent usefulness of these methods.},
  author	= {G. Manjunath and Herbert Jaeger},
  journal	= {Neural Computation},
  note		= {PMID: 23272918},
  number	= 3,
  pages		= {671-696},
  title		= {Echo State Property Linked to an Input: Exploring a
		  Fundamental Characteristic of Recurrent Neural Networks},
  url		= {http://doi.org/10.1162/NECO_a_00411},
  volume	= 25,
  year		= 2013
}

@InProceedings{	  MM+19,
  author	= {Pavlo Molchanov and Arun Mallya and Stephen Tyree and Iuri
		  Frosio and Jan Kautz},
  booktitle	= {IEEE/CVF Conference on Computer Vision and Pattern
		  Recognition (CVPR)},
  title		= {Importance Estimation for Neural Network Pruning},
  year		= 2019,
  pages		= {11256-11264},
  url		= {http://doi.org/10.1109/CVPR.2019.01152}
}

@Article{	  MNM02,
  author	= {Wolfgang Maass and Thomas Natschl{\"a}ger and Henry
		  Markram},
  journal	= {Neural Computation},
  number	= 11,
  pages		= {2531-2560},
  title		= {Real-Time Computing Without Stable States: A New Framework
		  for Neural Computation Based on Perturbations},
  url		= {http://doi.org/10.1162/089976602760407955},
  volume	= 14,
  year		= 2002
}

@InProceedings{	  MO+18,
  address	= {Nagoya, Japan},
  author	= {Olivia Michael and Oliver Obst and Falk Schmidsberger and
		  Frieder Stolzenburg},
  booktitle	= {RoboCup 2017: Robot Soccer World Cup XXI. RoboCup
		  International Symposium},
  editor	= {Hidehisa Akyama and Oliver Obst and Claude Sammut and
		  Flavio Tonidandel},
  pages		= {120-131},
  publisher	= {Springer Nature Switzerland},
  series	= {LNAI~11175},
  title		= {Analysing Soccer Games with Clustering and Conceptors},
  url		= {http://doi.org/10.1007/978-3-030-00308-1_10},
  year		= 2018
}

@InProceedings{	  MO+19,
  author	= {Olivia Michael and Oliver Obst and Falk Schmidsberger and
		  Frieder Stolzenburg},
  title		= {{R}obo{C}up{S}im{D}ata: Software and Data for Machine
		  Learning from {R}obo{C}up Simulation League},
  booktitle	= {RoboCup 2018: Robot Soccer World Cup XXII. RoboCup
		  International Symposium},
  year		= 2019,
  address	= {Montr{\'e}al, Canada},
  editor	= {Dirk Holz and Katie Genter and Maarouf Saad and Oskar von
		  Stryk},
  publisher	= {Springer Nature Switzerland},
  series	= {LNAI~11374},
  pages		= {230-237},
  url		= {http://doi.org/10.1007/978-3-030-27544-0_19}
}

@InProceedings{	  MS11,
  abstract	= {In this work we resolve the long-outstanding problem of
		  how to effectively train recurrent neural networks (RNNs)
		  on complex and difficult sequence modeling problems which
		  may contain long-term data dependencies. Utilizing recent
		  advances in the Hessian-free optimization approach
		  (Martens, 2010), together with a novel damping scheme, we
		  successfully train RNNs on two sets of challenging
		  problems. First, a collection of pathological synthetic
		  datasets which are known to be impossible for standard
		  optimization approaches (due to their extremely long-term
		  dependencies), and second, on three natural and highly
		  complex real-world sequence datasets where we find that our
		  method significantly outperforms the previous
		  state-of-the-art method for training neural sequence
		  models: the Long Short-term Memory approach of Hochreiter
		  and Schmidhuber (1997). Additionally, we offer a new
		  interpretation of the generalized Gauss-Newton matrix of
		  Schraudolph (2002) which is used within the HF approach of
		  Martens.},
  author	= {Martens, James and Sutskever, Ilya},
  booktitle	= {Proceedings of the 28th International Conference on
		  Machine Learning},
  pages		= {1033--1040},
  title		= {Learning Recurrent Neural Networks with {H}essian-free
		  Optimization},
  url		= {http://dl.acm.org/doi/10.5555/3104482.3104612},
  year		= {2011}
}

@TechReport{	  Nei18,
  address	= {Harz University of Applied Sciences},
  author	= {Rouven Neitzel},
  institution	= {Automation and Computer Sciences Department},
  note		= {In German},
  title		= {Prediction of sinusoidal signals by recurrent neural
		  networks},
  type		= {Project thesis},
  year		= 2018
}

@InProceedings{	  NPO17,
  title		= {Stock market's price movement prediction with LSTM neural
		  networks},
  author	= {David M. Q. Nelson and Adriano C. M. Pereira and Renato A.
		  de Oliveira },
  booktitle	= {International joint conference on neural networks
		  (IJCNN)},
  publisher	= {IEEE},
  year		= 2017,
  url		= {http://doi.org/10.1109/IJCNN.2017.7966019}
}

@TechReport{	  OTC15,
  author	= {Yann Ollivier and Corentin Tallec and Guillaume Charpiat},
  institution	= {Cornell University Library},
  number	= {abs/1507.07680},
  title		= {Training recurrent networks online without backtracking},
  type		= {{CoRR} -- Computing Research Repository},
  url		= {http://arxiv.org/abs/1507.07680},
  year		= 2015
}

@TechReport{	  PDW13,
  author	= {Hamid Palangi and Li Deng and Rabab K. Ward},
  institution	= {Cornell University Library},
  number	= {abs/1311.2987},
  title		= {Learning Input and Recurrent Weight Matrices in Echo State
		  Networks},
  type		= {{CoRR} -- Computing Research Repository},
  url		= {http://arxiv.org/abs/1311.2987},
  year		= 2013
}

@InProceedings{	  PGL17,
  author	= {Vitchyr Pong and Shixiang Gu and Sergey Levine},
  booktitle	= {Lifelong Learning: A Reinforcement Learning Approach
		  Workshop, International Conference on Machine Learning.},
  title		= {Learning Long-term Dependencies with Deep Memory States},
  year		= 2017,
  url		= {http://pdfs.semanticscholar.org/2e09/9bf26976e2334a9c4a2ad1aefc28cf83299b.pdf}
}

@Article{	  PMB13,
  address	= {Atlanta, Georgia},
  author	= {Razvan Pascanu and Tomas Mikolov and Yoshua Bengio},
  journal	= {Proceedings of the 30th International Conference on
		  Machine Learning},
  number	= 3,
  pages		= {1310-1318},
  title		= {On the difficulty of training recurrent neural networks},
  url		= {http://proceedings.mlr.press/v28/pascanu13.pdf},
  volume	= 28,
  year		= 2013
}

@InProceedings{	  PS14,
  title		= {Pre-training of Recurrent Neural Networks via Linear
		  Autoencoders},
  author	= {Luca Pasa and Alessandro Sperduti},
  booktitle	= {Advances in Neural Information Processing Systems 27 (NIPS
		  2014)},
  year		= 2014,
  pages		= {3572-3580},
  url		= {http://papers.nips.cc/paper/5271-pre-training-of-recurrent-neural-networks-via-linear-autoencoders}
}

@Article{	  Ree93,
  author	= {Russell Reed},
  journal	= {IEEE Transactions on Neural Networks},
  title		= {Pruning algorithms -- a survey},
  year		= 1993,
  volume	= 4,
  number	= 5,
  pages		= {740-747},
  doi		= {10.1109/72.248452},
  url		= {http://ieeexplore.ieee.org/document/248452}
}

@InProceedings{	  RK11,
  address	= {Berlin},
  author	= {Marco Ragni and Andreas Klein},
  booktitle	= {KI 2011: Advances in Artificial Intelligence --
		  Proceedings of the 34th Annual German Conference on
		  Artificial Intelligence},
  editor	= {Joscha Bach and Stefan Edelkamp},
  pages		= {255-259},
  publisher	= {Springer},
  series	= {LNAI~7006},
  title		= {Predicting Numbers: An {AI} Approach to Solving Number
		  Series},
  url		= {http://doi.org/10.1007/978-3-642-24455-1_24},
  year		= 2011
}

@Article{	  RPV17,
  author	= {Roondiwala, Murtaza and Harshal Patel and Shraddha Varma},
  title		= {Predicting stock prices using {LSTM}},
  journal	= {International Journal of Science and Research (IJSR)},
  year		= 2017,
  volume	= 6,
  number	= 4,
  pages		= {1754-1756},
  url		= {http://www.ijsr.net/archive/v6i4/ART20172755.pdf}
}

@Book{		  SB18,
  abstract	= {This introductory textbook on reinforcement learning is
		  targeted toward engineers and scientists in artificial
		  intelligence, operations research, neural networks, and
		  control systems, and we hope it will also be of interest to
		  psychologists and neuroscientists. It also contains several
		  new results. An html version is available.},
  author	= {Sutton, Richard S. and Barto, Andrew G.},
  publisher	= {MIT Press},
  title		= {Reinforcement Learning: An Introduction},
  edition	= {2nd},
  url		= {http://incompleteideas.net/book/the-book.html},
  year		= 2018
}

@Article{	  SHM+16,
  abstract	= {The game of Go has long been viewed as the most
		  challenging of classic games for artificial intelligence
		  owing to its enormous search space and the difficulty of
		  evaluating board positions and moves. Here we introduce a
		  new approach to computer Go that uses `value networks'to
		  evaluate board positions and `policy networks'to select
		  moves. These deep neural networks are trained by a novel
		  combination of supervised learning from human expert games,
		  and reinforcement learning from games of self-play. Without
		  any lookahead search, the neural networks play Go at the
		  level of state-of-the-art Monte Carlo tree search programs
		  that simulate thousands of random games of self-play. We
		  also introduce a new search algorithm that combines Monte
		  Carlo simulation with value and policy networks. Using this
		  search algorithm, our program AlphaGo achieved a 99.8{\%}
		  winning rate against other Go programs, and defeated the
		  human European Go champion by 5 games to 0. This is the
		  first time that a computer program has defeated a human
		  professional player in the full-sized game of Go, a feat
		  previously thought to be at least a decade away.},
  author	= {Silver, David and Huang, Aja and Maddison, Chris J. and
		  Guez, Arthur and Sifre, Laurent and van den Driessche,
		  George and Schrittwieser, Julian and Antonoglou, Ioannis
		  and Panneershelvam, Veda and Lanctot, Marc and Dieleman,
		  Sander and Grewe, Dominik and Nham, John and Kalchbrenner,
		  Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach,
		  Madeleine and Kavukcuoglu, Koray and Graepel, Thore and
		  Hassabis, Demis},
  journal	= {Nature},
  month		= jan,
  number	= {7587},
  pages		= {484-489},
  title		= {Mastering the game of {G}o with deep neural networks and
		  tree search},
  url		= {http://doi.org/10.1038/nature16961},
  volume	= {529},
  year		= 2016
}

@Article{	  SIZ19,
  author	= {Shah, Dev and Isah, Haruna and Zulkernine, Farhana},
  title		= {Stock Market Analysis: A Review and Taxonomy of Prediction
		  Techniques},
  journal	= {International Journal of Financial Studies},
  volume	= 7,
  year		= 2019,
  number	= 2,
  url		= {http://www.mdpi.com/2227-7072/7/2/26},
  issn		= {2227-7072},
  abstract	= {Stock market prediction has always caught the attention of
		  many analysts and researchers. Popular theories suggest
		  that stock markets are essentially a random walk and it is
		  a fool&rsquo;s game to try and predict them. Predicting
		  stock prices is a challenging problem in itself because of
		  the number of variables which are involved. In the short
		  term, the market behaves like a voting machine but in the
		  longer term, it acts like a weighing machine and hence
		  there is scope for predicting the market movements for a
		  longer timeframe. Application of machine learning
		  techniques and other algorithms for stock price analysis
		  and forecasting is an area that shows great promise. In
		  this paper, we first provide a concise review of stock
		  markets and taxonomy of stock market prediction methods. We
		  then focus on some of the research achievements in stock
		  analysis and prediction. We discuss technical, fundamental,
		  short- and long-term approaches used for stock analysis.
		  Finally, we present some challenges and research
		  opportunities in this field.},
  doi		= {10.3390/ijfs7020026}
}

@TechReport{	  SL+18,
  author	= {Frieder Stolzenburg and Sandra Litz and Olivia Michael and
		  Oliver Obst},
  title		= {The Power of Linear Recurrent Neural Networks},
  year		= 2018,
  institution	= {Cornell University Library},
  type		= {{CoRR} -- Computing Research Repository},
  number	= {abs/1802.03308},
  url		= {http://arxiv.org/abs/1802.03308},
  note		= {Latest revision 2020}
}

@InProceedings{	  SMO18,
  author	= {Frieder Stolzenburg and Olivia Michael and Oliver Obst},
  title		= {The Power of Linear Recurrent Neural Networks},
  booktitle	= {Cognitive Computing -- Merging Concepts with Hardware},
  year		= 2018,
  address	= {Hannover},
  editor	= {Daniel Brunner and Herbert Jaeger and Stuart Parkin and
		  Gordon Pipa},
  url		= {http://cognitive-comp.org/#topics},
  note		= {Received Prize for Most Technologically Feasible Poster
		  Contribution},
  place		= {ORDNER 2018}
}

@InProceedings{	  Spe06,
  title		= {Exact Solutions for Recursive Principal Components
		  Analysis of Sequences and Trees},
  author	= {Sperduti, Alessandro},
  editor	= {Kollias, Stefanos D. and Stafylopatis, Andreas and Duch,
		  W{\l}odzis{\l}aw and Oja, Erkki},
  booktitle	= {Artificial Neural Networks -- ICANN},
  url		= {http://link.springer.com/chapter/10.1007/11840817_37},
  year		= {2006},
  publisher	= {Springer},
  address	= {Berlin, Heidelberg},
  pages		= {349-356},
  abstract	= {We show how a family of exact solutions to the Recursive
		  Principal Components Analysis learning problem can be
		  computed for sequences and tree structured inputs. These
		  solutions are derived from eigenanalysis of extended
		  vectorial representations of the input structures and
		  substructures. Experimental results performed on sequences
		  and trees generated by a context-free grammar show the
		  effectiveness of the proposed approach.},
  isbn		= {978-3-540-38627-8}
}

@TechReport{	  Ste18,
  address	= {Harz University of Applied Sciences},
  author	= {Kai Steckhan},
  institution	= {Automation and Computer Sciences Department},
  note		= {In German},
  title		= {Time-series analysis with recurrent neural networks},
  type		= {Project thesis},
  year		= 2018
}

@InProceedings{	  Sto17b,
  address	= {Ghent, Belgium},
  author	= {Frieder Stolzenburg},
  booktitle	= {{ESCOM} 2017 -- 25th Anniversary Conference of the
		  European Society for the Cognitive Sciences of Music},
  editor	= {Van Dyck, Edith},
  note		= {Proceedings},
  pages		= {159-162},
  publisher	= {IPEM, Ghent University},
  title		= {Periodicity Detection by Neural Transformation},
  url		= {http://www.escom2017.org/wp-content/uploads/2016/06/Stolzenburg-et-al.pdf},
  year		= 2017,
  place		= {ORDNER 2016}
}

@Book{		  Str15,
  author	= {Steven H. Strogatz},
  title		= {Nonlinear Dynamics and Chaos. With Applications to
		  Physics, Biology, Chemistry, and Engneering},
  publisher	= {CRC Press},
  edition	= {2nd},
  year		= 2015,
  address	= {Boca Raton, FL},
  url		= {http://doi.org/10.1201/9780429492563}
}

@Article{	  SW+07,
  author	= {J{\"u}rgen Schmidhuber and Daan Wierstra and Matteo
		  Gagliolo and Faustino Gomez},
  issue		= 3,
  journal	= {Neural Computation},
  pages		= {757-779},
  title		= {Training Recurrent Networks by {E}volino},
  url		= {http://doi.org/10.1162/neco.2007.19.3.757},
  volume	= 19,
  year		= 2007
}

@Article{	  Tin18,
  abstract	= {We study asymptotic properties of Fisher memory of linear
		  Echo State Networks with randomized symmetric state space
		  coupling. In particular, two reservoir constructions are
		  considered: (1) More direct dynamic coupling construction
		  using a class of Wigner matrices and (2) positive
		  semi-definite dynamic coupling obtained as a product of
		  unconstrained stochastic matrices. We show that the maximal
		  Fisher memory is achieved when the input-to-state coupling
		  is collinear with the dominant eigenvector of the reservoir
		  coupling matrix. In the case of Wigner reservoirs we show
		  that as the system size grows, the contribution to the
		  Fisher memory of self-coupling of reservoir units is
		  negligible. We also prove that when the input-to-state
		  coupling is collinear with the sum of eigenvectors of the
		  state space coupling, the expected normalized memory is
		  four and eight time smaller than the maximal memory value
		  for the Wigner and product constructions, respectively.},
  author	= {Peter Ti{\v n}o},
  journal	= {Neurocomputing},
  keywords	= {Fisher memory of dynamical systems, Recurrent neural
		  network, Echo State Network, Reservoir Computing},
  pages		= {4-8},
  title		= {Asymptotic {F}isher memory of randomized linear symmetric
		  Echo State Networks},
  volume	= 298,
  year		= 2018,
  url		= {http://doi.org/10.1016/j.neucom.2017.11.076}
}

@Article{	  TV10,
  title		= {Random matrices: Universality of {ESD}s and the circular
		  law},
  author	= {Tao, Terence and Vu, Van and Krishnapur, Manjunath},
  journal	= {The Annals of Probability},
  volume	= 38,
  number	= 5,
  pages		= {2023-2065},
  year		= 2010,
  publisher	= {Institute of Mathematical Statistics},
  url		= {http://projecteuclid.org/euclid.aop/1282053780}
}

@InProceedings{	  VKE19,
  title		= {Legendre Memory Units: Continuous-Time Representation in
		  Recurrent Neural Networks},
  author	= {Aaron R. Voelker and Ivana Kaji{\'c} and Chris Eliasmith},
  booktitle	= {Advances in Neural Information Processing Systems 32
		  (NeurIPS 2019)},
  address	= {Vancouver, Canada},
  year		= 2019,
  url		= {http://proceedings.neurips.cc/paper/2019/file/952285b9b7e7a1be5aa7849f32ffff05-Paper.pdf}
}

@Article{	  WLS94,
  author	= {Olivia L. White and Daniel D. Lee and Haim Sompolinsky},
  journal	= {Physical Review Letters},
  number	= 14,
  pages		= {148102},
  title		= {Short-Term Memory in Orthogonal Neural Networks},
  url		= {http://journals.aps.org/prl/abstract/10.1103/PhysRevLett.92.148102},
  volume	= 92,
  year		= 1994
}

@Article{	  XYH07,
  author	= {Xue, Yanbo and Yang, Le and Haykin, Simon},
  journal	= {Neural Networks},
  number	= 3,
  pages		= {365-376},
  title		= {Decoupled echo state networks with lateral inhibition},
  url		= {http://doi.org/10.1016/j.neunet.2007.04.014},
  volume	= 20,
  year		= 2007
}

@Article{	  YJK12,
  author	= {Izzet B. Yildiz and Herbert Jaeger and Stefan J. Kiebel},
  journal	= {Neural Networks},
  pages		= {1-9},
  title		= {Re-visiting the echo state property},
  url		= {http://doi.org/10.1016/j.neunet.2012.07.005},
  volume	= 35,
  year		= 2012
}
